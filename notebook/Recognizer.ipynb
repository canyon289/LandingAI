{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landing AI + Ravin = :)\n",
    "\n",
    "The above message is pretty clear to me, but let's see if we can train a Neural Net to find that message in random noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The input to our neural net will be a set of images that either contain our  important message or a string of random characters. For extra difficulty the random AI will have a higher occurence of :) emojis, + and = characters as well. You will need to install the LandingAI package if following along.\n",
    "\n",
    "### Generate dataset\n",
    "Let's go ahead and generate some images. In the Landing AI package there the image class contains all the code used to generate test and train images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "train exists, deleting directory and images and creating new ones\n",
      "test exists, deleting directory and images and creating new ones\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from landingai import image\n",
    "image.gen_images(train_examples=5000, test_examples=200, delete=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load\n",
    "Each image now needs to be loaded and labeled, and the target vector needs to be one hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from landingai import model_utils\n",
    "x_train, y_train = model_utils.load_data(\"train\")\n",
    "x_test, y_test = model_utils.load_data(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encode Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, LeakyReLU\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 8, kernel_size=2, strides=2, padding='valid', input_shape=(270,270,1)))\n",
    "model.add(LeakyReLU(alpha =.1))\n",
    "model.add(MaxPooling2D(pool_size=4))\n",
    "\n",
    "model.add(Conv2D(filters = 4, kernel_size=2, strides=2, padding='valid'))\n",
    "model.add(LeakyReLU(alpha =.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 200 samples\n",
      "Epoch 1/20\n",
      "5000/5000 [==============================] - 18s - loss: 0.3787 - acc: 0.8822 - val_loss: 0.0885 - val_acc: 0.9950\n",
      "Epoch 2/20\n",
      "5000/5000 [==============================] - 18s - loss: 0.0394 - acc: 0.9934 - val_loss: 0.0102 - val_acc: 1.0000\n",
      "Epoch 3/20\n",
      "5000/5000 [==============================] - 20s - loss: 0.0204 - acc: 0.9928 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 4/20\n",
      "5000/5000 [==============================] - 20s - loss: 0.0115 - acc: 0.9960 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 5/20\n",
      "5000/5000 [==============================] - 20s - loss: 0.0108 - acc: 0.9956 - val_loss: 0.0249 - val_acc: 0.9950\n",
      "Epoch 6/20\n",
      "5000/5000 [==============================] - 21s - loss: 0.0066 - acc: 0.9976 - val_loss: 0.0792 - val_acc: 0.9650\n",
      "Epoch 7/20\n",
      "5000/5000 [==============================] - 25s - loss: 0.0080 - acc: 0.9964 - val_loss: 5.7241e-04 - val_acc: 1.0000\n",
      "Epoch 8/20\n",
      "5000/5000 [==============================] - 27s - loss: 0.0050 - acc: 0.9982 - val_loss: 3.2214e-04 - val_acc: 1.0000\n",
      "Epoch 9/20\n",
      "5000/5000 [==============================] - 24s - loss: 0.0059 - acc: 0.9982 - val_loss: 2.1440e-04 - val_acc: 1.0000\n",
      "Epoch 10/20\n",
      "5000/5000 [==============================] - 21s - loss: 0.0035 - acc: 0.9992 - val_loss: 2.4175e-04 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "5000/5000 [==============================] - 22s - loss: 0.0054 - acc: 0.9976 - val_loss: 9.0887e-04 - val_acc: 1.0000\n",
      "Epoch 12/20\n",
      "5000/5000 [==============================] - 23s - loss: 0.0045 - acc: 0.9982 - val_loss: 1.4521e-04 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "5000/5000 [==============================] - 22s - loss: 0.0055 - acc: 0.9980 - val_loss: 1.9710e-04 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "5000/5000 [==============================] - 23s - loss: 0.0077 - acc: 0.9976 - val_loss: 1.9327e-04 - val_acc: 1.0000\n",
      "Epoch 15/20\n",
      "5000/5000 [==============================] - 23s - loss: 0.0015 - acc: 0.9994 - val_loss: 9.9749e-05 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      "5000/5000 [==============================] - 21s - loss: 0.0023 - acc: 0.9994 - val_loss: 3.0231e-04 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "5000/5000 [==============================] - 21s - loss: 0.0056 - acc: 0.9986 - val_loss: 1.0985e-04 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "5000/5000 [==============================] - 22s - loss: 0.0043 - acc: 0.9986 - val_loss: 8.6224e-05 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "5000/5000 [==============================] - 22s - loss: 0.0017 - acc: 0.9994 - val_loss: 5.0491e-04 - val_acc: 1.0000\n",
      "Epoch 20/20\n",
      "5000/5000 [==============================] - 22s - loss: 0.0020 - acc: 0.9990 - val_loss: 3.5478e-05 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f439fe6bc18>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs = 20, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"Classifier.hdf5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
